{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed: int = 42\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device:str = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(text:str, seq_length:int, vocab_to_idx:dict):\n",
    "    text_length = len(text)\n",
    "    while True:\n",
    "        for i in range(0, text_length - seq_length, seq_length):\n",
    "            input_text = text[i:i + seq_length]\n",
    "            target_text = text[i + 1:i + seq_length + 1]\n",
    "\n",
    "            input_ids = torch.tensor([vocab_to_idx[char] for char in input_text], dtype=torch.long)\n",
    "            target_ids = torch.tensor([vocab_to_idx[char] for char in target_text], dtype=torch.long)\n",
    "\n",
    "            yield input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "URL = \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\"\n",
    "response = requests.get(URL)\n",
    "\n",
    "if response.status_code != 200:\n",
    "    exit()\n",
    "\n",
    "text = response.text\n",
    "\n",
    "text[:60]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab:set[str] = sorted(set(text))\n",
    "vocab_to_idx:dict[str, int] = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_vocab:dict = {idx: char for char, idx in vocab_to_idx.items()}\n",
    "\n",
    "seq_length:int = 300\n",
    "generator = text_generator(text, seq_length, vocab_to_idx)\n",
    "BATCH_SIZE = 64\n",
    "def collate_fn(batch:list):\n",
    "    inputs, targets = zip(*batch)\n",
    "    inputs = torch.stack(inputs)\n",
    "    targets = torch.stack(targets)\n",
    "    return inputs, targets\n",
    "\n",
    "dataloader = DataLoader(generator, batch_size=BATCH_SIZE, collate_fn=collate_fn, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size:int, embedding_dim:int, rnn_units:int):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, rnn_units, batch_first=True)\n",
    "        self.fc = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size:int = len(vocab)\n",
    "embedding_dim:int = 256\n",
    "rnn_units:int = 1024\n",
    "\n",
    "model = RNNModel(vocab_size, embedding_dim, rnn_units).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, generator, criterion, optimizer, epochs:int, batch_size:int) -> None:\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        hidden = None\n",
    "        step = 0\n",
    "        while step < (len(text) // (seq_length * batch_size)):\n",
    "            step += 1\n",
    "            batch = [next(generator) for _ in range(batch_size)]\n",
    "            inputs, targets = zip(*batch)\n",
    "            inputs = torch.stack(inputs).to(device)\n",
    "            targets = torch.stack(targets).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs, hidden = model(inputs, hidden)\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "            loss = criterion(outputs.view(-1, vocab_size), targets.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Step {step}, Loss: {loss.item():.4f}\", end='\\r')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Step 58, Loss: 2.1285\n",
      "Epoch 2/30, Step 58, Loss: 1.9055\n",
      "Epoch 3/30, Step 58, Loss: 1.7853\n",
      "Epoch 4/30, Step 58, Loss: 1.6942\n",
      "Epoch 5/30, Step 58, Loss: 1.6193\n",
      "Epoch 6/30, Step 58, Loss: 1.5565\n",
      "Epoch 7/30, Step 58, Loss: 1.5054\n",
      "Epoch 8/30, Step 58, Loss: 1.4666\n",
      "Epoch 9/30, Step 58, Loss: 1.4426\n",
      "Epoch 10/30, Step 58, Loss: 1.4055\n",
      "Epoch 11/30, Step 58, Loss: 1.3664\n",
      "Epoch 12/30, Step 58, Loss: 1.3351\n",
      "Epoch 13/30, Step 58, Loss: 1.2896\n",
      "Epoch 14/30, Step 58, Loss: 1.2616\n",
      "Epoch 15/30, Step 58, Loss: 1.2114\n",
      "Epoch 16/30, Step 58, Loss: 1.1794\n",
      "Epoch 17/30, Step 58, Loss: 1.1371\n",
      "Epoch 18/30, Step 58, Loss: 1.1038\n",
      "Epoch 19/30, Step 58, Loss: 1.0688\n",
      "Epoch 20/30, Step 58, Loss: 1.0486\n",
      "Epoch 21/30, Step 58, Loss: 1.0260\n",
      "Epoch 22/30, Step 58, Loss: 0.9971\n",
      "Epoch 23/30, Step 58, Loss: 0.9814\n",
      "Epoch 24/30, Step 58, Loss: 0.9575\n",
      "Epoch 25/30, Step 58, Loss: 0.9292\n",
      "Epoch 26/30, Step 58, Loss: 0.9093\n",
      "Epoch 27/30, Step 58, Loss: 0.8927\n",
      "Epoch 28/30, Step 58, Loss: 0.8974\n",
      "Epoch 29/30, Step 58, Loss: 0.9018\n",
      "Epoch 30/30, Step 58, Loss: 0.8932\n"
     ]
    }
   ],
   "source": [
    "EPOCHS:int = 30\n",
    "train_model(model, generator, criterion, optimizer, EPOCHS, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "save_path = \"model.pth\"\n",
    "torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_text:str, length:int, idx_to_vocab:dict[int, str], vocab_to_idx:dict[str, int], temperature:float=1.0) -> str:\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor([vocab_to_idx[char] for char in start_text], dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = None\n",
    "\n",
    "    generated_text = start_text\n",
    "    print(generated_text, end=\"\")\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            outputs, hidden = model(input_ids, hidden)\n",
    "            logits = outputs[:, -1, :] / temperature\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        next_char = idx_to_vocab[next_id]\n",
    "        print(next_char, end=\"\")\n",
    "\n",
    "        generated_text += next_char\n",
    "\n",
    "        input_ids = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "\n",
      "Ghost of Frand, I would make me such a preck.\n",
      "\n",
      "MONTAGUE:\n",
      "You must pawns by thee, to this gettle Gaunt;\n",
      "For devil's called consideres\n",
      "Thoughts are not, sir:\n",
      "I'll tell her my hand.\n",
      "\n",
      "PAULINA:\n",
      "I hope my senseless; but is no meet.\n",
      "\n",
      "LUCENTIO:\n",
      "Go to, go to:\n",
      "Gold Bohen!--\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Mend an edact: a ring to seem thee.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "KING EDWARD IV:\n",
      "Lest he had been aside,\n",
      "And the summon'd of the vews front; I shall\n",
      "It steal my assurance made me burable:\n",
      "I must have show'd to-morrow, giddy, fellow;\n",
      "So falsed as youbsteed, old favour\n",
      "Inspurn a sudden will take her hence:\n",
      "And yet makes for me.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "ISABELLA:\n",
      "What, ho! what will betroked?\n",
      "But in pluck him and left me sich a good\n",
      "which sire; contains God tell her maid.\n",
      "\n",
      "FADY CAPULET:\n",
      "Verona, some son of liberty.\n",
      "\n",
      "LUCIO:\n",
      "\n",
      "ISABELLA:\n",
      "Who is't that were I appay?\n",
      "\n",
      "LUCIO:\n",
      "I pray.\n",
      "\n",
      "LUCIO:\n",
      "Have you no none here? mether.\n",
      "\n",
      "Provost:\n",
      "Do stop there, fool, if I tell me\n",
      "Af our prisoner? Camillo!\n",
      "\n",
      "Aft think me: there is,\n",
      "Unless abroad unconsider'd "
     ]
    }
   ],
   "source": [
    "start_text:str = \"ROMEO:\"\n",
    "generated_text:str = generate_text(model, start_text, 1000, idx_to_vocab, vocab_to_idx, 1.0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
