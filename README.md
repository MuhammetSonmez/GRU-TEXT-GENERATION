# Text Generation with GRU

## Project Description
This project demonstrates text generation using a **Gated Recurrent Unit (GRU)** model built with **PyTorch**. The model is trained on a given text dataset to generate coherent and contextually relevant sentences.

## Features
- Uses **GRU-based Recurrent Neural Networks** for text generation.
- Implements **character-level and word-level generation**.
- Provides **configurable training parameters** such as epochs, learning rate, and batch size.
- Includes **data preprocessing, training, and text generation scripts**.

## Requirements
- Python 3.10+
- PyTorch 2.5.1+
- NumPy
- Matplotlib (for visualization)
- Jupyter Notebook (optional for interactive usage)

## How It Works
1. The input text dataset is preprocessed and tokenized.
2. The GRU-based model is trained to learn character or word sequences.
3. The trained model generates text based on a given seed input.

## Contributing
Contributions are welcome! Feel free to fork the repository and submit pull requests with improvements or new features.

